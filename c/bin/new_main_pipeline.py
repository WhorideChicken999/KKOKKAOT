#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìƒˆë¡œìš´ ë©”ì¸ íŒŒì´í”„ë¼ì¸
1. ì „ì²´ ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ ì˜ˆì¸¡ (22ê°œ ìŠ¤íƒ€ì¼)
2. YOLOë¡œ ìƒì˜/í•˜ì˜/ì•„ìš°í„°/ë“œë ˆìŠ¤ í¬ë¡­ (confidence 0.5 ì´ìƒ)
3. í¬ë¡­ëœ ì´ë¯¸ì§€ë³„ ì¹´í…Œê³ ë¦¬ ì†ì„± ì˜ˆì¸¡
4. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë° ChromaDB ì„ë² ë”©
"""

import cv2
import numpy as np
import torch
import torch.nn as nn
from PIL import Image
from pathlib import Path
from ultralytics import YOLO
from transformers import CLIPProcessor, CLIPModel
from torchvision import models, transforms
import chromadb
import psycopg2
from psycopg2.extras import Json
import json
import pandas as pd
from typing import Dict, Tuple, Optional, List
import warnings
warnings.filterwarnings('ignore')

# GPU ì„¤ì •
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {DEVICE}")

class NewFashionPipeline:
    """ìƒˆë¡œìš´ íŒ¨ì…˜ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, 
                style_model_path: str = "D:/kkokkaot/API/pre_trained_weights/k_fashion_best_model.pth",
                yolo_detection_path: str = "D:/kkokkaot/API/pre_trained_weights/yolo_best.pt",
                category_models_dir: str = "D:/kkokkaot/API/pre_trained_weights/category_attributes",
                schema_path: str = "D:/kkokkaot/API/kfashion_attributes_schema.csv",
                chroma_path: str = "./chroma_db",
                db_config: dict = None):
        """ì´ˆê¸°í™”"""
        
        print("\n=== ìƒˆë¡œìš´ íŒ¨ì…˜ íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ë¡œë”© ì¤‘ ===")
        
        # 1. ìŠ¤íƒ€ì¼ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ
        print("1. ìŠ¤íƒ€ì¼ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ...")
        self.style_model, self.style_classes = self.load_style_model(style_model_path)
        
        # 2. YOLO Detection ëª¨ë¸ ë¡œë“œ
        print("2. YOLO Detection ëª¨ë¸ ë¡œë“œ...")
        self.yolo_detection_model = YOLO(yolo_detection_path)
        
        # 3. ì¹´í…Œê³ ë¦¬ë³„ ì†ì„± ëª¨ë¸ ë¡œë“œ
        print("3. ì¹´í…Œê³ ë¦¬ë³„ ì†ì„± ëª¨ë¸ ë¡œë“œ...")
        self.category_models = self.load_category_models(category_models_dir)
        
        # 4. ìŠ¤í‚¤ë§ˆ ë¡œë“œ
        print("4. ì†ì„± ìŠ¤í‚¤ë§ˆ ë¡œë“œ...")
        self.schema = self.load_schema(schema_path)
        
        # 5. CLIP ëª¨ë¸ (ì„ë² ë”©ìš©)
        print("5. CLIP ëª¨ë¸ ë¡œë“œ...")
        self.clip_model, self.clip_processor = self.load_clip_model()
        
        # 6. ChromaDB
        print("6. ChromaDB ì—°ê²°...")
        self.chroma_client = chromadb.PersistentClient(path=chroma_path)
        try:
            self.chroma_collection = self.chroma_client.get_collection(name="fashion_collection")
        except:
            self.chroma_collection = self.chroma_client.create_collection(name="fashion_collection")
        
        # 7. PostgreSQL
        print("7. PostgreSQL ì—°ê²°...")
        if db_config is None:
            db_config = {
                'host': 'localhost',
                'port': 5432,
                'database': 'kkokkaot_closet',
                'user': 'postgres',
                'password': '000000'
            }
        self.db_config = db_config
        self.db_conn = psycopg2.connect(**db_config)
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        print("\nâœ“ ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")
    
    def load_style_model(self, model_path: str):
        """ìŠ¤íƒ€ì¼ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ"""
        try:
            checkpoint = torch.load(model_path, map_location=DEVICE, weights_only=False)
            
            # ëª¨ë¸ êµ¬ì¡° ì¶”ì • (ì¼ë°˜ì ì¸ ResNet ê¸°ë°˜)
            num_classes = 22  # 22ê°œ ìŠ¤íƒ€ì¼
            model = models.resnet50(pretrained=False)
            model.fc = nn.Linear(model.fc.in_features, num_classes)
            model.load_state_dict(checkpoint)
            model.to(DEVICE)
            model.eval()
            
            # ìŠ¤íƒ€ì¼ í´ë˜ìŠ¤ ì •ì˜ (ìŠ¤í‚¤ë§ˆì—ì„œ ì¶”ì¶œ)
            style_classes = [
                'ë¡œë§¨í‹±', 'í˜ë¯¸ë‹Œ', 'ì„¹ì‹œ', 'ì  ë”ë¦¬ìŠ¤/ì  ë”í”Œë£¨ì´ë“œ', 'ë§¤ìŠ¤í˜ë¦°', 'í†°ë³´ì´',
                'íˆí”¼', 'ì˜¤ë¦¬ì—”íƒˆ', 'ì›¨ìŠ¤í„´', 'ì»¨íŠ¸ë¦¬', 'ë¦¬ì¡°íŠ¸', 'ëª¨ë˜',
                'ì†Œí”¼ìŠ¤íŠ¸ì¼€ì´í‹°ë“œ', 'ì•„ë°©ê°€ë¥´ë“œ', 'í‘í¬', 'í‚¤ì¹˜/í‚¤ëœíŠ¸', 'ë ˆíŠ¸ë¡œ',
                'í™í•©', 'í´ë˜ì‹', 'í”„ë ˆí”¼', 'ìŠ¤íŠ¸ë¦¬íŠ¸', 'ë°€ë¦¬í„°ë¦¬', 'ìŠ¤í¬í‹°'
            ]
            
            return model, style_classes
            
        except Exception as e:
            print(f"âŒ ìŠ¤íƒ€ì¼ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None
    
    def load_category_models(self, models_dir: str):
        """ì¹´í…Œê³ ë¦¬ë³„ ì†ì„± ëª¨ë¸ ë¡œë“œ"""
        models_dir = Path(models_dir)
        category_models = {}
        
        categories = ['ìƒì˜', 'í•˜ì˜', 'ì•„ìš°í„°', 'ì›í”¼ìŠ¤']
        
        for category in categories:
            category_models[category] = {}
            
            # ê° ì†ì„±ë³„ ëª¨ë¸ ë¡œë“œ
            attributes = ['ì¹´í…Œê³ ë¦¬', 'ìƒ‰ìƒ', 'í•', 'ì†Œì¬', 'ê¸°ì¥', 'ì†Œë§¤ê¸°ì¥', 'ë„¥ë¼ì¸', 'í”„ë¦°íŠ¸']
            
            for attribute in attributes:
                model_file = models_dir / f"best_model_{category}_{attribute}.pth"
                info_file = models_dir / f"model_info_{category}_{attribute}.json"
                
                if model_file.exists() and info_file.exists():
                    try:
                        # ëª¨ë¸ ì •ë³´ ë¡œë“œ
                        with open(info_file, 'r', encoding='utf-8') as f:
                            model_info = json.load(f)
                        
                        # ëª¨ë¸ ë¡œë“œ
                        model = CategoryAttributeCNN(model_info['num_classes'])
                        model.load_state_dict(torch.load(model_file, map_location=DEVICE))
                        model.to(DEVICE)
                        model.eval()
                        
                        category_models[category][attribute] = {
                            'model': model,
                            'num_classes': model_info['num_classes'],
                            'class_names': model_info['class_names']
                        }
                        
                        print(f"  âœ“ {category}_{attribute} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                        
                    except Exception as e:
                        print(f"  âŒ {category}_{attribute} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue
                else:
                    print(f"  âš ï¸ {category}_{attribute} ëª¨ë¸ íŒŒì¼ ì—†ìŒ")
        
        return category_models
    
    def load_schema(self, schema_path: str):
        """ì†ì„± ìŠ¤í‚¤ë§ˆ ë¡œë“œ"""
        try:
            schema_df = pd.read_csv(schema_path)
            return schema_df
        except Exception as e:
            print(f"âŒ ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def load_clip_model(self):
        """CLIP ëª¨ë¸ ë¡œë“œ"""
        try:
            clip_model = CLIPModel.from_pretrained("patrickjohncyh/fashion-clip").to(DEVICE)
            clip_processor = CLIPProcessor.from_pretrained("patrickjohncyh/fashion-clip")
        except:
            clip_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14").to(DEVICE)
            clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
        
        clip_model.eval()
        return clip_model, clip_processor
    
    def predict_style(self, image: np.ndarray) -> Dict:
        """ì „ì²´ ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ ì˜ˆì¸¡"""
        print("[1/7] ì „ì²´ ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ ì˜ˆì¸¡ ì¤‘...")
        
        if self.style_model is None:
            return {'style': 'Unknown', 'confidence': 0.0}
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        pil_image = Image.fromarray(image)
        image_tensor = self.transform(pil_image).unsqueeze(0).to(DEVICE)
        
        with torch.no_grad():
            outputs = self.style_model(image_tensor)
            probs = torch.softmax(outputs, dim=1)[0]
            style_idx = probs.argmax().item()
            confidence = probs[style_idx].item()
            style = self.style_classes[style_idx]
        
        result = {
            'style': style,
            'confidence': confidence
        }
        
        print(f"  - ìŠ¤íƒ€ì¼: {style} ({confidence:.2f})")
        return result
    
    def detect_and_crop_categories(self, image_path: str) -> Dict:
        """YOLOë¡œ ì¹´í…Œê³ ë¦¬ ê°ì§€ ë° í¬ë¡­"""
        print("[2/7] YOLO ì¹´í…Œê³ ë¦¬ ê°ì§€ ë° í¬ë¡­ ì¤‘...")
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # YOLO Detection ì¶”ë¡ 
        results = self.yolo_detection_model(image_path, verbose=False)
        
        detected_items = {
            'ìƒì˜': [],
            'í•˜ì˜': [],
            'ì•„ìš°í„°': [],
            'ì›í”¼ìŠ¤': []
        }
        
        # í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘
        class_names = ['ì•„ìš°í„°', 'ìƒì˜', 'í•˜ì˜', 'ì›í”¼ìŠ¤']
        category_mapping = {
            'outer': 'ì•„ìš°í„°',
            'top': 'ìƒì˜',
            'bottom': 'í•˜ì˜',
            'dress': 'ì›í”¼ìŠ¤'
        }
        
        if len(results[0].boxes) > 0:
            boxes = results[0].boxes
            for i, box in enumerate(boxes):
                class_id = int(box.cls[0])
                confidence = float(box.conf[0])
                
                # confidence 0.5 ì´ìƒë§Œ ì²˜ë¦¬
                if confidence >= 0.5 and class_id < len(class_names):
                    class_name_en = class_names[class_id]
                    class_name_ko = category_mapping.get(class_name_en)
                    
                    if class_name_ko:
                        # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                        
                        # ì´ë¯¸ì§€ í¬ë¡­
                        cropped_image = image_rgb[y1:y2, x1:x2]
                        
                        detected_items[class_name_ko].append({
                            'bbox': (x1, y1, x2, y2),
                            'confidence': confidence,
                            'cropped_image': cropped_image
                        })
                        
                        print(f"  - {class_name_ko}: confidence={confidence:.2f}, bbox=({x1},{y1},{x2},{y2})")
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê°€ì¥ ë†’ì€ confidence ì„ íƒ
        final_items = {}
        for category, items in detected_items.items():
            if items:
                best_item = max(items, key=lambda x: x['confidence'])
                final_items[category] = best_item
                print(f"  âœ… {category} ì„ íƒ: confidence={best_item['confidence']:.2f}")
        
        return {
            'original': image_rgb,
            'detected_items': final_items,
            'has_ìƒì˜': 'ìƒì˜' in final_items,
            'has_í•˜ì˜': 'í•˜ì˜' in final_items,
            'has_ì•„ìš°í„°': 'ì•„ìš°í„°' in final_items,
            'has_ì›í”¼ìŠ¤': 'ì›í”¼ìŠ¤' in final_items
        }
    
    def predict_category_attributes(self, category: str, cropped_image: np.ndarray) -> Dict:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ì†ì„± ì˜ˆì¸¡"""
        print(f"  [3/7] {category} ì†ì„± ì˜ˆì¸¡ ì¤‘...")
        
        if category not in self.category_models:
            return {}
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        pil_image = Image.fromarray(cropped_image)
        image_tensor = self.transform(pil_image).unsqueeze(0).to(DEVICE)
        
        attributes = {}
        
        # ê° ì†ì„±ë³„ ì˜ˆì¸¡
        for attribute, model_info in self.category_models[category].items():
            try:
                model = model_info['model']
                class_names = model_info['class_names']
                
                with torch.no_grad():
                    outputs = model(image_tensor)
                    probs = torch.softmax(outputs, dim=1)[0]
                    pred_idx = probs.argmax().item()
                    confidence = probs[pred_idx].item()
                    predicted_class = class_names[pred_idx]
                    
                    attributes[attribute] = {
                        'value': predicted_class,
                        'confidence': confidence
                    }
                    
                    print(f"    - {attribute}: {predicted_class} ({confidence:.2f})")
                    
            except Exception as e:
                print(f"    âŒ {category}_{attribute} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                continue
        
        return attributes
    
    def create_embedding(self, image: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±"""
        print("[4/7] ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        pil_image = Image.fromarray(image)
        
        inputs = self.clip_processor(
            images=pil_image,
            return_tensors="pt",
            padding=True
        ).to(DEVICE)
        
        with torch.no_grad():
            image_features = self.clip_model.get_image_features(**inputs)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        
        embedding = image_features.cpu().numpy()[0]
        print(f"  - ì„ë² ë”© ì°¨ì›: {embedding.shape}")
        
        return embedding
    
    def save_to_postgresql(self, user_id: int, image_path: str, 
                          style_result: Dict, detection_result: Dict,
                          category_attributes: Dict, chroma_id: str = None) -> int:
        """PostgreSQLì— ì €ì¥"""
        print("[5/7] PostgreSQLì— ì €ì¥ ì¤‘...")
        
        try:
            self.db_conn.rollback()
        except:
            pass
        
        try:
            with self.db_conn.cursor() as cur:
                # wardrobe_items ì‚½ì…
                cur.execute("""
                    INSERT INTO wardrobe_items (
                        user_id, original_image_path, style, style_confidence,
                        has_top, has_bottom, has_outer, has_dress,
                        chroma_embedding_id
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                    RETURNING item_id
                """, (
                    user_id, 
                    image_path,
                    style_result['style'],
                    style_result['confidence'],
                    detection_result['has_ìƒì˜'],
                    detection_result['has_í•˜ì˜'],
                    detection_result['has_ì•„ìš°í„°'],
                    detection_result['has_ì›í”¼ìŠ¤'],
                    chroma_id
                ))
                
                item_id = cur.fetchone()[0]
                
                # ê° ì¹´í…Œê³ ë¦¬ë³„ ì†ì„± ì €ì¥
                for category, attributes in category_attributes.items():
                    if attributes:
                        # ì¹´í…Œê³ ë¦¬ë³„ í…Œì´ë¸”ì— ì €ì¥
                        table_name = f"{category.lower()}_attributes"
                        
                        # ì†ì„± ê°’ ì¶”ì¶œ
                        category_val = attributes.get('ì¹´í…Œê³ ë¦¬', {}).get('value', 'Unknown')
                        color_val = attributes.get('ìƒ‰ìƒ', {}).get('value', 'Unknown')
                        fit_val = attributes.get('í•', {}).get('value', 'Unknown')
                        material_val = attributes.get('ì†Œì¬', {}).get('value', 'Unknown')
                        length_val = attributes.get('ê¸°ì¥', {}).get('value', 'Unknown')
                        sleeve_length_val = attributes.get('ì†Œë§¤ê¸°ì¥', {}).get('value', 'Unknown')
                        neckline_val = attributes.get('ë„¥ë¼ì¸', {}).get('value', 'Unknown')
                        print_val = attributes.get('í”„ë¦°íŠ¸', {}).get('value', 'Unknown')
                        
                        # ì‹ ë¢°ë„ ì¶”ì¶œ
                        category_conf = attributes.get('ì¹´í…Œê³ ë¦¬', {}).get('confidence', 0.0)
                        color_conf = attributes.get('ìƒ‰ìƒ', {}).get('confidence', 0.0)
                        fit_conf = attributes.get('í•', {}).get('confidence', 0.0)
                        
                        cur.execute(f"""
                            INSERT INTO {table_name} (
                                item_id, category, color, fit, material, length,
                                sleeve_length, neckline, print_pattern,
                                category_confidence, color_confidence, fit_confidence
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        """, (
                            item_id, category_val, color_val, fit_val, material_val,
                            length_val, sleeve_length_val, neckline_val, print_val,
                            category_conf, color_conf, fit_conf
                        ))
                
                self.db_conn.commit()
                
            print(f"  - ì•„ì´í…œ ID: {item_id}")
            return item_id
            
        except Exception as e:
            try:
                self.db_conn.rollback()
                print(f"  âŒ PostgreSQL ì €ì¥ ì‹¤íŒ¨ (rollback ì™„ë£Œ): {e}")
            except:
                print(f"  âŒ PostgreSQL ì €ì¥ ì‹¤íŒ¨: {e}")
            raise e
    
    def save_to_chromadb(self, item_id: int, embedding: np.ndarray, 
                        metadata: Dict) -> str:
        """ChromaDBì— ì €ì¥"""
        print("[6/7] ChromaDBì— ì €ì¥ ì¤‘...")
        
        chroma_id = f"item_{item_id}"
        
        # ë©”íƒ€ë°ì´í„°ë¥¼ ë¬¸ì„œë¡œ ë³€í™˜
        doc_parts = []
        if metadata.get('style'):
            doc_parts.append(f"ìŠ¤íƒ€ì¼: {metadata['style']}")
        
        for category in ['ìƒì˜', 'í•˜ì˜', 'ì•„ìš°í„°', 'ì›í”¼ìŠ¤']:
            if metadata.get(f'{category}_category'):
                doc_parts.append(f"{category}: {metadata[f'{category}_category']}")
        
        document = " | ".join(doc_parts)
        
        self.chroma_collection.add(
            ids=[chroma_id],
            embeddings=[embedding.tolist()],
            metadatas=[metadata],
            documents=[document]
        )
        
        print(f"  - Chroma ID: {chroma_id}")
        return chroma_id
    
    def search_similar(self, embedding: np.ndarray, n_results: int = 5) -> list:
        """ìœ ì‚¬ ì•„ì´í…œ ê²€ìƒ‰"""
        print(f"[7/7] ìœ ì‚¬ ì•„ì´í…œ ê²€ìƒ‰ ì¤‘ (Top {n_results})...")
        
        results = self.chroma_collection.query(
            query_embeddings=[embedding.tolist()],
            n_results=n_results
        )
        
        print(f"  - ê²€ìƒ‰ ì™„ë£Œ: {len(results['ids'][0])}ê°œ")
        return results
    
    def process_image(self, image_path: str, user_id: int) -> Dict:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print(f"\n{'='*60}")
        print(f"ìƒˆë¡œìš´ íŒ¨ì…˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {image_path}")
        print(f"{'='*60}\n")
        
        try:
            # 1. ìŠ¤íƒ€ì¼ ì˜ˆì¸¡
            image = cv2.imread(image_path)
            if image is None:
                raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            style_result = self.predict_style(image_rgb)
            
            # 2. ì¹´í…Œê³ ë¦¬ ê°ì§€ ë° í¬ë¡­
            detection_result = self.detect_and_crop_categories(image_path)
            
            # 3. ê° ì¹´í…Œê³ ë¦¬ë³„ ì†ì„± ì˜ˆì¸¡
            category_attributes = {}
            for category, item in detection_result['detected_items'].items():
                cropped_image = item['cropped_image']
                attributes = self.predict_category_attributes(category, cropped_image)
                category_attributes[category] = attributes
            
            # 4. ì„ë² ë”© ìƒì„±
            embedding = self.create_embedding(image_rgb)
            
            # 5. ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadata = {
                'user_id': str(user_id),
                'style': style_result['style'],
                'style_confidence': style_result['confidence']
            }
            
            for category, attributes in category_attributes.items():
                for attr_name, attr_data in attributes.items():
                    metadata[f'{category}_{attr_name}'] = attr_data['value']
                    metadata[f'{category}_{attr_name}_confidence'] = attr_data['confidence']
            
            # 6. PostgreSQL ì €ì¥
            item_id = self.save_to_postgresql(
                user_id, image_path, style_result, detection_result,
                category_attributes, chroma_id=None
            )
            
            # 7. ChromaDB ì €ì¥
            chroma_id = self.save_to_chromadb(item_id, embedding, metadata)
            
            # 8. PostgreSQLì— chroma_id ì—…ë°ì´íŠ¸
            with self.db_conn.cursor() as cur:
                cur.execute("""
                    UPDATE wardrobe_items 
                    SET chroma_embedding_id = %s 
                    WHERE item_id = %s
                """, (chroma_id, item_id))
                self.db_conn.commit()
            
            # 9. ìœ ì‚¬ ì•„ì´í…œ ê²€ìƒ‰
            similar_items = self.search_similar(embedding, n_results=5)
            
            print(f"\n{'='*60}")
            print(f"âœ” íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            print(f"  - ì•„ì´í…œ ID: {item_id}")
            print(f"  - Chroma ID: {chroma_id}")
            print(f"  - ìŠ¤íƒ€ì¼: {style_result['style']}")
            
            detected_categories = list(detection_result['detected_items'].keys())
            print(f"  - ê°ì§€ëœ ì˜ë¥˜: {', '.join(detected_categories)}")
            
            for category, attributes in category_attributes.items():
                if attributes:
                    print(f"  - {category}:")
                    for attr_name, attr_data in attributes.items():
                        print(f"    {attr_name}: {attr_data['value']}")
            
            print(f"{'='*60}\n")
            
            return {
                'success': True,
                'item_id': item_id,
                'chroma_id': chroma_id,
                'style_result': style_result,
                'detection_result': detection_result,
                'category_attributes': category_attributes,
                'similar_items': similar_items
            }
            
        except Exception as e:
            print(f"\nâœ— ì—ëŸ¬ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            
            try:
                if hasattr(self, 'db_conn') and self.db_conn:
                    self.db_conn.rollback()
            except:
                pass
            
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_image_info(self, item_id: int) -> Dict:
        """ì´ë¯¸ì§€ ì •ë³´ ì¡°íšŒ (ì „ì²´ ì´ë¯¸ì§€ìš©)"""
        try:
            with self.db_conn.cursor() as cur:
                cur.execute("""
                    SELECT item_id, original_image_path, style, style_confidence,
                           has_top, has_bottom, has_outer, has_dress
                    FROM wardrobe_items 
                    WHERE item_id = %s
                """, (item_id,))
                
                result = cur.fetchone()
                if result:
                    return {
                        'item_id': result[0],
                        'image_path': result[1],
                        'style': result[2],
                        'style_confidence': result[3],
                        'has_top': result[4],
                        'has_bottom': result[5],
                        'has_outer': result[6],
                        'has_dress': result[7]
                    }
                else:
                    return None
                    
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def get_category_info(self, item_id: int, category: str) -> Dict:
        """ì¹´í…Œê³ ë¦¬ë³„ ì†ì„± ì •ë³´ ì¡°íšŒ (í¬ë¡­ëœ ì´ë¯¸ì§€ìš©)"""
        try:
            table_name = f"{category.lower()}_attributes"
            
            with self.db_conn.cursor() as cur:
                cur.execute(f"""
                    SELECT category, color, fit, material, length,
                           sleeve_length, neckline, print_pattern,
                           category_confidence, color_confidence, fit_confidence
                    FROM {table_name} 
                    WHERE item_id = %s
                """, (item_id,))
                
                result = cur.fetchone()
                if result:
                    return {
                        'category': result[0],
                        'color': result[1],
                        'fit': result[2],
                        'material': result[3],
                        'length': result[4],
                        'sleeve_length': result[5],
                        'neckline': result[6],
                        'print_pattern': result[7],
                        'category_confidence': result[8],
                        'color_confidence': result[9],
                        'fit_confidence': result[10]
                    }
                else:
                    return None
                    
        except Exception as e:
            print(f"âŒ {category} ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        if self.db_conn:
            self.db_conn.close()
            print("PostgreSQL ì—°ê²° ì¢…ë£Œ")


class CategoryAttributeCNN(nn.Module):
    """ì¹´í…Œê³ ë¦¬ë³„ ì†ì„± ë¶„ë¥˜ CNN ëª¨ë¸"""
    
    def __init__(self, num_classes, pretrained=True):
        super(CategoryAttributeCNN, self).__init__()
        
        # ResNet50 ë°±ë³¸ ì‚¬ìš©
        self.backbone = models.resnet50(pretrained=pretrained)
        num_features = self.backbone.fc.in_features
        
        # ê¸°ì¡´ fc ë ˆì´ì–´ ì œê±°
        self.backbone.fc = nn.Identity()
        
        # ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Sequential(
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        features = self.backbone(x)
        output = self.classifier(features)
        return output


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¯ ìƒˆë¡œìš´ íŒ¨ì…˜ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = NewFashionPipeline()
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì²˜ë¦¬
    test_image_path = "test_image.jpg"  # ì‹¤ì œ ì´ë¯¸ì§€ ê²½ë¡œë¡œ ë³€ê²½
    user_id = 1
    
    result = pipeline.process_image(test_image_path, user_id)
    
    if result['success']:
        print("âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ!")
    else:
        print("âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨!")
    
    pipeline.close()


if __name__ == "__main__":
    main()
