#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SAM ëª¨ë¸ì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë° ëˆ„ë¼ë”°ê¸°
YOLOë¡œ ê°ì§€ëœ bbox ì˜ì—­ì„ SAMìœ¼ë¡œ ì •í™•íˆ ì„¸ê·¸ë©˜í…Œì´ì…˜í•˜ì—¬ ë°°ê²½ ì œê±°
"""

import os
import cv2
import numpy as np
import torch
from pathlib import Path
import json
from PIL import Image, ImageDraw
import matplotlib.pyplot as plt
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# SAM2 ê´€ë ¨ import
try:
    from sam2.build_sam import build_sam2
    from sam2.sam2_image_predictor import SAM2ImagePredictor
    SAM2_AVAILABLE = True
except ImportError:
    print("âš ï¸ sam2ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ë°©ë²•: pip install git+https://github.com/facebookresearch/segment-anything-2.git")
    SAM2_AVAILABLE = False

class SAM2Segmentation:
    """SAM2 ëª¨ë¸ì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ í´ë˜ìŠ¤"""
    
    def __init__(self, sam_model_path, config_path, device="cuda"):
        """
        SAM2 ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            sam_model_path: SAM2 ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ
            config_path: SAM2 ì„¤ì • íŒŒì¼ ê²½ë¡œ
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ("cuda" ë˜ëŠ” "cpu")
        """
        if not SAM2_AVAILABLE:
            raise ImportError("sam2 ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        self.device = device if torch.cuda.is_available() else "cpu"
        print(f"ğŸš€ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {self.device}")
        
        # SAM2 ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ“¦ SAM2 ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.sam2 = build_sam2(sam2_cfg_file=config_path, 
                              sam2_ckpt_path=sam_model_path, 
                              device=self.device)
        
        # SAM2 ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”
        self.predictor = SAM2ImagePredictor(self.sam2)
        print("âœ… SAM2 ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def segment_bbox(self, image, bbox, point_coords=None, point_labels=None):
        """
        bbox ì˜ì—­ì„ ì„¸ê·¸ë©˜í…Œì´ì…˜
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€ (numpy array)
            bbox: ë°”ìš´ë”© ë°•ìŠ¤ [x1, y1, x2, y2]
            point_coords: í¬ì¸íŠ¸ ì¢Œí‘œ (ì„ íƒì‚¬í•­)
            point_labels: í¬ì¸íŠ¸ ë¼ë²¨ (ì„ íƒì‚¬í•­)
        
        Returns:
            mask: ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬
        """
        # ì´ë¯¸ì§€ ì„¤ì •
        self.predictor.set_image(image)
        
        # bboxë¥¼ SAM í˜•ì‹ìœ¼ë¡œ ë³€í™˜ [x1, y1, x2, y2] -> [x, y, w, h]
        x1, y1, x2, y2 = bbox
        sam_bbox = np.array([x1, y1, x2, y2])
        
        # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜ˆì¸¡
        masks, scores, logits = self.predictor.predict(
            point_coords=point_coords,
            point_labels=point_labels,
            box=sam_bbox,
            multimask_output=True,
        )
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë§ˆìŠ¤í¬ ì„ íƒ
        best_mask = masks[np.argmax(scores)]
        
        return best_mask, scores[np.argmax(scores)]
    
    def create_transparent_image(self, image, mask, alpha=1.0):
        """
        ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ íˆ¬ëª… ë°°ê²½ ì´ë¯¸ì§€ ìƒì„±
        
        Args:
            image: ì›ë³¸ ì´ë¯¸ì§€
            mask: ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬
            alpha: íˆ¬ëª…ë„ (0.0 ~ 1.0)
        
        Returns:
            transparent_image: íˆ¬ëª… ë°°ê²½ ì´ë¯¸ì§€ (RGBA)
        """
        # ì´ë¯¸ì§€ë¥¼ RGBAë¡œ ë³€í™˜
        if len(image.shape) == 3:
            if image.shape[2] == 3:
                # RGB -> RGBA
                rgba_image = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)
                rgba_image[:, :, :3] = image
                rgba_image[:, :, 3] = 255  # ì•ŒíŒŒ ì±„ë„ì„ 255ë¡œ ì„¤ì •
            else:
                rgba_image = image.copy()
        else:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ -> RGBA
            rgba_image = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)
            rgba_image[:, :, :3] = np.stack([image] * 3, axis=2)
            rgba_image[:, :, 3] = 255
        
        # ë§ˆìŠ¤í¬ë¥¼ ì•ŒíŒŒ ì±„ë„ì— ì ìš©
        rgba_image[:, :, 3] = (mask * 255 * alpha).astype(np.uint8)
        
        return rgba_image
    
    def save_segmented_image(self, image, mask, output_path, format="PNG"):
        """
        ì„¸ê·¸ë©˜í…Œì´ì…˜ëœ ì´ë¯¸ì§€ ì €ì¥
        
        Args:
            image: ì›ë³¸ ì´ë¯¸ì§€
            mask: ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬
            output_path: ì €ì¥ ê²½ë¡œ
            format: ì €ì¥ í˜•ì‹ ("PNG", "JPG")
        """
        if format.upper() == "PNG":
            # íˆ¬ëª… ë°°ê²½ìœ¼ë¡œ PNG ì €ì¥
            transparent_image = self.create_transparent_image(image, mask)
            pil_image = Image.fromarray(transparent_image, 'RGBA')
            pil_image.save(output_path, "PNG")
        else:
            # ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•œ ì´ë¯¸ì§€ ì €ì¥
            masked_image = image.copy()
            masked_image[~mask] = [0, 0, 0]  # ë°°ê²½ì„ ê²€ì€ìƒ‰ìœ¼ë¡œ
            pil_image = Image.fromarray(masked_image)
            pil_image.save(output_path, "JPEG")

class YOLOSAM2Processor:
    """YOLO + SAM2ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, yolo_model_path, sam_model_path, sam_config_path):
        """
        YOLO + SAM2 í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        
        Args:
            yolo_model_path: YOLO ëª¨ë¸ ê²½ë¡œ
            sam_model_path: SAM2 ëª¨ë¸ ê²½ë¡œ
            sam_config_path: SAM2 ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        # YOLO ëª¨ë¸ ë¡œë“œ
        print("ğŸ“¦ YOLO ëª¨ë¸ ë¡œë”© ì¤‘...")
        try:
            self.yolo_model = torch.hub.load('ultralytics/yolov5', 'custom', path=yolo_model_path)
            print("âœ… YOLO ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ YOLO ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.yolo_model = None
        
        # SAM2 ëª¨ë¸ ë¡œë“œ
        self.sam2_segmenter = SAM2Segmentation(sam_model_path, sam_config_path)
        
        # í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘ (YOLO ëª¨ë¸ì— ë”°ë¼ ì¡°ì • í•„ìš”)
        self.class_names = {
            0: 'top',
            1: 'bottom', 
            2: 'outer',
            3: 'dress'
        }
    
    def detect_and_segment(self, image_path, output_dir, confidence_threshold=0.3):
        """
        ì´ë¯¸ì§€ì—ì„œ ê°ì²´ë¥¼ ê°ì§€í•˜ê³  ì„¸ê·¸ë©˜í…Œì´ì…˜
        
        Args:
            image_path: ì…ë ¥ ì´ë¯¸ì§€ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            confidence_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
        
        Returns:
            results: ì²˜ë¦¬ ê²°ê³¼ ì •ë³´
        """
        if self.yolo_model is None:
            print("âŒ YOLO ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(str(image_path))
        if image is None:
            print(f"âŒ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            return None
        
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # YOLOë¡œ ê°ì²´ ê°ì§€
        results = self.yolo_model(image_rgb)
        detections = results.pandas().xyxy[0]
        
        # ì‹ ë¢°ë„ í•„í„°ë§
        detections = detections[detections['confidence'] >= confidence_threshold]
        
        if len(detections) == 0:
            print(f"âš ï¸ ê°ì§€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            return None
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì´ë¯¸ì§€ ì´ë¦„ (í™•ì¥ì ì œì™¸)
        image_name = Path(image_path).stem
        
        processed_results = []
        
        # ê° ê°ì§€ëœ ê°ì²´ì— ëŒ€í•´ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰
        for idx, detection in detections.iterrows():
            # bbox ì¢Œí‘œ
            x1, y1, x2, y2 = detection['xmin'], detection['ymin'], detection['xmax'], detection['ymax']
            bbox = [int(x1), int(y1), int(x2), int(y2)]
            
            # í´ë˜ìŠ¤ ì •ë³´
            class_id = int(detection['class'])
            class_name = self.class_names.get(class_id, f'class_{class_id}')
            confidence = detection['confidence']
            
            print(f"  ğŸ¯ {class_name} ê°ì§€ (ì‹ ë¢°ë„: {confidence:.3f})")
            
            # SAMìœ¼ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜
            try:
                mask, score = self.sam_segmenter.segment_bbox(image_rgb, bbox)
                
                # ì„¸ê·¸ë©˜í…Œì´ì…˜ëœ ì´ë¯¸ì§€ ì €ì¥
                output_filename = f"{image_name}_{class_name}_{idx}.png"
                output_path = output_dir / output_filename
                
                self.sam_segmenter.save_segmented_image(
                    image_rgb, mask, output_path, format="PNG"
                )
                
                processed_results.append({
                    'class_name': class_name,
                    'class_id': class_id,
                    'confidence': confidence,
                    'bbox': bbox,
                    'mask_score': score,
                    'output_path': str(output_path)
                })
                
                print(f"    âœ… ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ: {output_filename}")
                
            except Exception as e:
                print(f"    âŒ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
                continue
        
        return processed_results
    
    def process_batch(self, input_dir, output_dir, confidence_threshold=0.3):
        """
        ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ ì´ë¯¸ì§€ ì²˜ë¦¬
        
        Args:
            input_dir: ì…ë ¥ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            confidence_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
        """
        input_dir = Path(input_dir)
        output_dir = Path(output_dir)
        
        # ì§€ì›í•˜ëŠ” ì´ë¯¸ì§€ í™•ì¥ì
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
        
        # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
        image_files = []
        for ext in image_extensions:
            image_files.extend(input_dir.glob(f"*{ext}"))
            image_files.extend(input_dir.glob(f"*{ext.upper()}"))
        
        if len(image_files) == 0:
            print(f"âŒ ì…ë ¥ ë””ë ‰í† ë¦¬ì— ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
            return
        
        print(f"ğŸ“Š ì²˜ë¦¬í•  ì´ë¯¸ì§€: {len(image_files)}ê°œ")
        
        # ë°°ì¹˜ ì²˜ë¦¬
        all_results = []
        for image_file in tqdm(image_files, desc="ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘"):
            print(f"\nğŸ”„ ì²˜ë¦¬ ì¤‘: {image_file.name}")
            
            results = self.detect_and_segment(
                image_file, output_dir, confidence_threshold
            )
            
            if results:
                all_results.extend(results)
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"  - ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {len(image_files)}ê°œ")
        print(f"  - ìƒì„±ëœ ì„¸ê·¸ë©˜í…Œì´ì…˜: {len(all_results)}ê°œ")
        
        # í´ë˜ìŠ¤ë³„ í†µê³„
        class_counts = {}
        for result in all_results:
            class_name = result['class_name']
            class_counts[class_name] = class_counts.get(class_name, 0) + 1
        
        print(f"  - í´ë˜ìŠ¤ë³„ í†µê³„:")
        for class_name, count in class_counts.items():
            print(f"    {class_name}: {count}ê°œ")
        
        return all_results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ YOLO + SAM ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œì‘!")
    print("=" * 60)
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    yolo_model_path = "API/pre_trained_weights/yolo_best.pt"
    sam_model_path = "API/pre_trained_weights/sam2_best.pt"
    sam_config_path = "API/pre_trained_weights/sam2_hiera_t.yaml"
    
    # ê²½ë¡œ í™•ì¸
    if not Path(yolo_model_path).exists():
        print(f"âŒ YOLO ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {yolo_model_path}")
        return
    
    if not Path(sam_model_path).exists():
        print(f"âŒ SAM2 ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sam_model_path}")
        return
    
    if not Path(sam_config_path).exists():
        print(f"âŒ SAM2 ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sam_config_path}")
        return
    
    # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    try:
        processor = YOLOSAM2Processor(yolo_model_path, sam_model_path, sam_config_path)
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # ì²˜ë¦¬í•  ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì„¤ì •
    input_dirs = [
        "converted_data/all_images",  # converted_dataì˜ ì´ë¯¸ì§€ë“¤
        "API/uploaded_images",        # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë“¤
        "API/default_items"           # ê¸°ë³¸ ì•„ì´í…œë“¤
    ]
    
    output_base_dir = "segmented_images"
    
    # ê° ì…ë ¥ ë””ë ‰í† ë¦¬ì— ëŒ€í•´ ì²˜ë¦¬
    for input_dir in input_dirs:
        input_path = Path(input_dir)
        if not input_path.exists():
            print(f"âš ï¸ ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_dir}")
            continue
        
        print(f"\nğŸ“ ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬: {input_dir}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_dir = Path(output_base_dir) / input_path.name
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
        results = processor.process_batch(
            input_path, 
            output_dir, 
            confidence_threshold=0.3
        )
        
        if results:
            # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
            results_file = output_dir / "segmentation_results.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {results_file}")
    
    print(f"\nğŸ‰ ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ğŸ“ ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼: {output_base_dir}/")

if __name__ == "__main__":
    main()
