"""
ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸
- ì „ì²´ íë¦„: ì´ë¯¸ì§€ ì…ë ¥ â†’ Gender â†’ Style â†’ YOLO â†’ Crop â†’ ì†ì„± ì˜ˆì¸¡
"""
import torch
import cv2
import numpy as np
from PIL import Image
from pathlib import Path
from torchvision import transforms
from typing import Dict, Tuple, Optional
from .models import STYLE_CLASSES, GENDER_CLASSES


class FashionPredictor:
    """íŒ¨ì…˜ ì•„ì´í…œ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, model_loader):
        """
        Args:
            model_loader: ModelLoader ì¸ìŠ¤í„´ìŠ¤
        """
        self.loader = model_loader
        self.device = model_loader.device
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])
    
    def predict_gender(self, image: np.ndarray) -> Dict:
        """
        1ë‹¨ê³„: ì„±ë³„ ì˜ˆì¸¡ (ì „ì²´ ì´ë¯¸ì§€)
        
        Args:
            image: RGB ì´ë¯¸ì§€ (numpy array)
        
        Returns:
            {'gender': 'male', 'confidence': 0.95}
        """
        print("\n[1/6] ì„±ë³„ ì˜ˆì¸¡ ì¤‘...")
        
        if self.loader.gender_model is None:
            print("  âš ï¸ ì„±ë³„ ëª¨ë¸ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
            return {'gender': 'female', 'confidence': 0.5}
        
        try:
            # ì „ì²˜ë¦¬
            pil_image = Image.fromarray(image)
            image_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)
            
            # ì˜ˆì¸¡
            with torch.no_grad():
                outputs = self.loader.gender_model(image_tensor)
                probs = torch.softmax(outputs, dim=1)[0]
                gender_idx = probs.argmax().item()
                confidence = probs[gender_idx].item()
                gender = GENDER_CLASSES[gender_idx]
            
            result = {
                'gender': gender,
                'confidence': confidence
            }
            
            print(f"  âœ… ì„±ë³„: {gender} ({confidence:.2%})")
            return result
            
        except Exception as e:
            print(f"  âŒ ì„±ë³„ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {'gender': 'female', 'confidence': 0.0}
    
    def predict_style(self, image: np.ndarray) -> Dict:
        """
        2ë‹¨ê³„: ìŠ¤íƒ€ì¼ ì˜ˆì¸¡ (ì „ì²´ ì´ë¯¸ì§€)
        
        Args:
            image: RGB ì´ë¯¸ì§€
        
        Returns:
            {'style': 'ìŠ¤íŠ¸ë¦¬íŠ¸', 'confidence': 0.89}
        """
        print("\n[2/6] ìŠ¤íƒ€ì¼ ì˜ˆì¸¡ ì¤‘...")
        
        if self.loader.style_model is None:
            print("  âš ï¸ ìŠ¤íƒ€ì¼ ëª¨ë¸ ì—†ìŒ")
            return {'style': 'Unknown', 'confidence': 0.0}
        
        try:
            # ì „ì²˜ë¦¬
            pil_image = Image.fromarray(image)
            image_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)
            
            # ì˜ˆì¸¡
            with torch.no_grad():
                outputs = self.loader.style_model(image_tensor)
                probs = torch.softmax(outputs, dim=1)[0]
                style_idx = probs.argmax().item()
                confidence = probs[style_idx].item()
                style = STYLE_CLASSES[style_idx]
            
            result = {
                'style': style,
                'confidence': confidence
            }
            
            print(f"  âœ… ìŠ¤íƒ€ì¼: {style} ({confidence:.2%})")
            return result
            
        except Exception as e:
            print(f"  âŒ ìŠ¤íƒ€ì¼ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {'style': 'Unknown', 'confidence': 0.0}
    
    def detect_items(self, image_path: str) -> Dict:
        """
        3ë‹¨ê³„: YOLO ë””í…íŒ… (ìƒì˜/í•˜ì˜/ì•„ìš°í„°/ì›í”¼ìŠ¤)
        
        Args:
            image_path: ì´ë¯¸ì§€ ê²½ë¡œ
        
        Returns:
            {
                'detected_items': {
                    'top': {'bbox': (x1,y1,x2,y2), 'confidence': 0.9, 'cropped_image': np.array},
                    'bottom': {...},
                },
                'has_top': True,
                'has_bottom': True,
                'has_outer': False,
                'has_dress': False
            }
        """
        print("\n[3/6] YOLO ë””í…íŒ… ì¤‘...")
        
        if self.loader.yolo_model is None:
            print("  âš ï¸ YOLO ëª¨ë¸ ì—†ìŒ")
            return {'detected_items': {}, 'has_top': False, 'has_bottom': False, 'has_outer': False, 'has_dress': False}
        
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = cv2.imread(image_path)
            if image is None:
                raise ValueError(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
            
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # YOLO ì¶”ë¡ 
            results = self.loader.yolo_model(image_path, verbose=False)
            
            # í´ë˜ìŠ¤ ë§¤í•‘ (YOLO ëª¨ë¸ì˜ í´ë˜ìŠ¤ ìˆœì„œ)
            class_names = ['outer', 'top', 'bottom', 'dress']
            category_mapping = {
                'outer': 'outer',
                'top': 'top',
                'bottom': 'bottom',
                'dress': 'dress'
            }
            
            detected_items = {
                'top': [],
                'bottom': [],
                'outer': [],
                'dress': []
            }
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ì¶œ
            if len(results[0].boxes) > 0:
                boxes = results[0].boxes
                print(f"  ğŸ“¦ ê°ì§€ëœ ë°•ìŠ¤: {len(boxes)}ê°œ")
                
                for box in boxes:
                    class_id = int(box.cls[0])
                    confidence = float(box.conf[0])
                    
                    if confidence >= 0.3 and class_id < len(class_names):
                        class_name = class_names[class_id]
                        category = category_mapping.get(class_name)
                        
                        if category:
                            # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                            
                            # ìœ íš¨ì„± ê²€ì‚¬
                            if x1 >= 0 and y1 >= 0 and x2 > x1 and y2 > y1:
                                # ì´ë¯¸ì§€ í¬ë¡­
                                cropped = image_rgb[y1:y2, x1:x2]
                                
                                if cropped.size > 0:
                                    detected_items[category].append({
                                        'bbox': (x1, y1, x2, y2),
                                        'confidence': confidence,
                                        'cropped_image': cropped
                                    })
                                    print(f"  âœ… {category}: conf={confidence:.2%}, bbox=({x1},{y1},{x2},{y2})")
            
            # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê°€ì¥ ë†’ì€ confidence ì„ íƒ
            final_items = {}
            for category, items in detected_items.items():
                if items:
                    best_item = max(items, key=lambda x: x['confidence'])
                    final_items[category] = best_item
            
            return {
                'detected_items': final_items,
                'has_top': 'top' in final_items,
                'has_bottom': 'bottom' in final_items,
                'has_outer': 'outer' in final_items,
                'has_dress': 'dress' in final_items
            }
            
        except Exception as e:
            print(f"  âŒ YOLO ë””í…íŒ… ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return {'detected_items': {}, 'has_top': False, 'has_bottom': False, 'has_outer': False, 'has_dress': False}
    
    def save_cropped_images(self, detected_items: Dict, user_id: int, item_id: int, save_dir: Path) -> Dict:
        """
        4ë‹¨ê³„: Cropëœ ì´ë¯¸ì§€ ì €ì¥
        
        Args:
            detected_items: detect_items()ì˜ ê²°ê³¼
            user_id: ì‚¬ìš©ì ID
            item_id: ì•„ì´í…œ ID
            save_dir: ì €ì¥ ê²½ë¡œ
        
        Returns:
            {'top': 'path/to/top.jpg', 'bottom': 'path/to/bottom.jpg', ...}
        """
        print("\n[4/6] Crop ì´ë¯¸ì§€ ì €ì¥ ì¤‘...")
        
        saved_paths = {}
        
        for category, item in detected_items.items():
            try:
                # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
                category_dir = save_dir / f"user_{user_id}" / category
                category_dir.mkdir(parents=True, exist_ok=True)
                
                # íŒŒì¼ ì €ì¥
                filename = f"item_{item_id}_{category}.jpg"
                file_path = category_dir / filename
                
                # numpy array â†’ PIL Image â†’ ì €ì¥
                cropped_image = item['cropped_image']
                pil_image = Image.fromarray(cropped_image)
                pil_image.save(file_path)
                
                saved_paths[category] = str(file_path)
                print(f"  âœ… {category}: {file_path}")
                
            except Exception as e:
                print(f"  âŒ {category} ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return saved_paths
    
    def predict_attributes(self, detected_items: Dict, predicted_gender: str) -> Dict:
        """
        5ë‹¨ê³„: ê° Cropë³„ ì†ì„± ì˜ˆì¸¡
        
        Args:
            detected_items: detect_items()ì˜ ê²°ê³¼
            predicted_gender: 1ë‹¨ê³„ì—ì„œ ì˜ˆì¸¡í•œ ì„±ë³„ ('male' or 'female')
        
        Returns:
            {
                'top': {'category': 'Tì…”ì¸ ', 'color': 'ê²€ì •', 'gender': 'male', ...},
                'bottom': {...},
            }
        """
        print(f"\n[5/6] ì†ì„± ì˜ˆì¸¡ ì¤‘... (gender={predicted_gender})")
        
        all_attributes = {}
        
        for category, item in detected_items.items():
            try:
                if category not in self.loader.attribute_models:
                    print(f"  âš ï¸ {category} ëª¨ë¸ ì—†ìŒ")
                    continue
                
                model_info = self.loader.attribute_models[category]
                model = model_info['model']
                encoders = model_info['encoders']
                attributes_list = model_info['attributes']
                
                # ì „ì²˜ë¦¬
                cropped_image = item['cropped_image']
                pil_image = Image.fromarray(cropped_image)
                image_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)
                
                # ì˜ˆì¸¡
                with torch.no_grad():
                    outputs = model(image_tensor)
                    
                    category_attrs = {}
                    
                    for attr in attributes_list:
                        if attr == 'gender':
                            # genderëŠ” ì˜ˆì¸¡í•˜ì§€ ì•Šê³  ì´ë¯¸ ì˜ˆì¸¡í•œ ê°’ ì‚¬ìš©
                            category_attrs['gender'] = {
                                'value': predicted_gender,
                                'confidence': 1.0  # ì „ì²´ ì´ë¯¸ì§€ì—ì„œ ì˜ˆì¸¡í•œ ê°’ì´ë¯€ë¡œ ì‹ ë¢°ë„ ë†’ìŒ
                            }
                        elif attr in outputs:
                            # ë‹¤ë¥¸ ì†ì„±ì€ ëª¨ë¸ë¡œ ì˜ˆì¸¡
                            attr_output = outputs[attr]
                            probs = torch.softmax(attr_output, dim=1)[0]
                            pred_idx = probs.argmax().item()
                            confidence = probs[pred_idx].item()
                            
                            # ë””ì½”ë”©
                            predicted_class = encoders[attr].inverse_transform([pred_idx])[0]
                            
                            category_attrs[attr] = {
                                'value': predicted_class,
                                'confidence': confidence
                            }
                    
                    all_attributes[category] = category_attrs
                    print(f"  âœ… {category}: {len(category_attrs)}ê°œ ì†ì„± ì˜ˆì¸¡ ì™„ë£Œ")
                
            except Exception as e:
                print(f"  âŒ {category} ì†ì„± ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
        
        return all_attributes
    
    def process_image(self, image_path: str, user_id: int, save_dir: Path = Path("./processed_images")) -> Dict:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            image_path: ì…ë ¥ ì´ë¯¸ì§€ ê²½ë¡œ
            user_id: ì‚¬ìš©ì ID
            save_dir: ì €ì¥ ê²½ë¡œ
        
        Returns:
            {
                'success': True,
                'gender': {'gender': 'male', 'confidence': 0.95},
                'style': {'style': 'ìŠ¤íŠ¸ë¦¬íŠ¸', 'confidence': 0.89},
                'detection': {...},
                'saved_paths': {...},
                'attributes': {
                    'top': {'category': 'Tì…”ì¸ ', 'gender': 'male', ...},
                    'bottom': {...}
                }
            }
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¯ íŒ¨ì…˜ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print(f"  - ì´ë¯¸ì§€: {image_path}")
        print(f"  - ì‚¬ìš©ì: {user_id}")
        print(f"{'='*60}")
        
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = cv2.imread(image_path)
            if image is None:
                raise ValueError(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # 1. Gender ì˜ˆì¸¡ (ì „ì²´ ì´ë¯¸ì§€)
            gender_result = self.predict_gender(image_rgb)
            predicted_gender = gender_result['gender']
            
            # 2. Style ì˜ˆì¸¡ (ì „ì²´ ì´ë¯¸ì§€)
            style_result = self.predict_style(image_rgb)
            
            # 3. YOLO ë””í…íŒ…
            detection_result = self.detect_items(image_path)
            
            if not detection_result['detected_items']:
                print("\nâŒ ì˜ë¥˜ ê°ì§€ ì‹¤íŒ¨")
                return {
                    'success': False,
                    'error': 'ì˜ë¥˜ë¥¼ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'
                }
            
            # 4. Crop ì´ë¯¸ì§€ ì €ì¥ (ì„ì‹œ item_id ì‚¬ìš©, DB ì €ì¥ í›„ ì—…ë°ì´íŠ¸)
            # saved_paths = self.save_cropped_images(
            #     detection_result['detected_items'],
            #     user_id,
            #     item_id=0,  # ì„ì‹œ ID
            #     save_dir=save_dir
            # )
            
            # 5. ì†ì„± ì˜ˆì¸¡ (gender í¬í•¨)
            attributes = self.predict_attributes(
                detection_result['detected_items'],
                predicted_gender  # ğŸ‘ˆ 1ë‹¨ê³„ì—ì„œ ì˜ˆì¸¡í•œ gender ì „ë‹¬
            )
            
            print(f"\n{'='*60}")
            print(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
            print(f"{'='*60}\n")
            
            return {
                'success': True,
                'gender': gender_result,
                'style': style_result,
                'detection': detection_result,
                # 'saved_paths': saved_paths,
                'attributes': attributes
            }
            
        except Exception as e:
            print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'success': False,
                'error': str(e)
            }

