#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¹´í…Œê³ ë¦¬ë³„ ì†ì„± ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì½”ë“œ
prepared_dataì˜ ë¶„ë¦¬ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì¹´í…Œê³ ë¦¬ë³„ ì†ì„± ë¶„ë¥˜ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
"""

import os
import json
import numpy as np
import pandas as pd
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ì„¤ì •
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class CategoryAttributeDataset(Dataset):
    """ì¹´í…Œê³ ë¦¬ë³„ ì†ì„± ë¶„ë¥˜ ë°ì´í„°ì…‹"""
    
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜ {image_path}: {e}")
            # ë¹ˆ ì´ë¯¸ì§€ ìƒì„±
            image = Image.new('RGB', (224, 224), (128, 128, 128))
        
        if self.transform:
            image = self.transform(image)
        
        label = self.labels[idx]
        return image, label

class CategoryAttributeCNN(nn.Module):
    """ì¹´í…Œê³ ë¦¬ë³„ ì†ì„± ë¶„ë¥˜ CNN ëª¨ë¸"""
    
    def __init__(self, num_classes, pretrained=True):
        super(CategoryAttributeCNN, self).__init__()
        
        # ResNet50 ë°±ë³¸ ì‚¬ìš©
        self.backbone = models.resnet50(pretrained=pretrained)
        num_features = self.backbone.fc.in_features  # ResNet50: 2048
        
        # ê¸°ì¡´ fc ë ˆì´ì–´ ì œê±°
        self.backbone.fc = nn.Identity()
        
        # ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Sequential(
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        features = self.backbone(x)
        output = self.classifier(features)
        return output

class CategoryAttributeTrainer:
    """ì¹´í…Œê³ ë¦¬ë³„ ì†ì„± í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, data_dir="D:/converted_data/prepared_data", output_dir="D:/kkokkaot/API/pre_trained_weights/category_attributes"):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ê²½ë¡œ
        self.cnn_labels_dir = self.data_dir / "cnn_labels"
        self.cropped_images_dir = self.data_dir / "cropped_images"
        
        # ëª¨ë¸ ì €ì¥ì†Œ
        self.models = {}
        self.label_encoders = {}
        self.training_results = {}
        
        print(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬: {self.data_dir}")
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        print(f"ğŸ“ CNN ë¼ë²¨ ë””ë ‰í† ë¦¬: {self.cnn_labels_dir}")
        print(f"ğŸ“ í¬ë¡­ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {self.cropped_images_dir}")
    
    def load_category_data(self, category):
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ë°ì´í„° ë¡œë“œ"""
        print(f"\nğŸ”„ {category} ë°ì´í„° ë¡œë”©...")
        
        category_cnn_dir = self.cnn_labels_dir / category
        category_images_dir = self.cropped_images_dir / category
        
        if not category_cnn_dir.exists() or not category_images_dir.exists():
            print(f"âŒ {category} ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
            return None, None
        
        # CNN ë¼ë²¨ íŒŒì¼ë“¤ ë¡œë“œ
        print("  ğŸ“‚ CNN ë¼ë²¨ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        cnn_files = list(category_cnn_dir.glob("*.json"))
        print(f"ğŸ“Š {category} CNN ë¼ë²¨ íŒŒì¼: {len(cnn_files)}ê°œ")
        
        # ì´ë¯¸ì§€ íŒŒì¼ë“¤ í™•ì¸
        print("  ğŸ–¼ï¸ í¬ë¡­ ì´ë¯¸ì§€ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        image_files = list(category_images_dir.glob("*.jpg"))
        print(f"ğŸ“Š {category} í¬ë¡­ ì´ë¯¸ì§€: {len(image_files)}ê°œ")
        
        # ë°ì´í„° ë§¤ì¹­ - ì •í™•í•œ íŒŒì¼ëª… ë§¤ì¹­ (ìµœì í™”)
        print("  ğŸ”„ íŒŒì¼ ë§¤ì¹­ ì¤‘...")
        
        # ì´ë¯¸ì§€ íŒŒì¼ëª…ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ë¹ ë¥¸ ê²€ìƒ‰
        image_dict = {img.stem: img for img in image_files}
        print(f"  ğŸ“ ì´ë¯¸ì§€ ë”•ì…”ë„ˆë¦¬ ìƒì„± ì™„ë£Œ: {len(image_dict)}ê°œ")
        
        valid_data = []
        unmatched_count = 0
        
        # ì§„í–‰ë°” ì¶”ê°€
        cnn_pbar = tqdm(cnn_files, desc="  ğŸ“‚ CNN íŒŒì¼ ë§¤ì¹­", ncols=100)
        for cnn_file in cnn_pbar:
            try:
                with open(cnn_file, 'r', encoding='utf-8') as f:
                    cnn_data = json.load(f)
                
                image_id = cnn_data['image_id']
                item_data = cnn_data['item_data']
                
                # CNN íŒŒì¼ëª…ì—ì„œ ì •í™•í•œ ë§¤ì¹­ ì´ë¯¸ì§€ ì°¾ê¸° (ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©)
                cnn_stem = cnn_file.stem  # ì˜ˆ: "1110355_ìƒì˜_001"
                matching_image = image_dict.get(cnn_stem)
                
                # ë§¤ì¹­ë˜ëŠ” ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if matching_image:
                    valid_data.append({
                        'image_path': str(matching_image),
                        'item_data': item_data
                    })
                else:
                    unmatched_count += 1
                    # ë§¤ì¹­ë˜ì§€ ì•ŠëŠ” íŒŒì¼ ë¡œê·¸ (ì²˜ìŒ 5ê°œë§Œ)
                    if unmatched_count <= 5:
                        print(f"  âš ï¸ ë§¤ì¹­ ì´ë¯¸ì§€ ì—†ìŒ: {cnn_stem}")
                
                # ì§„í–‰ë°” ì—…ë°ì´íŠ¸
                cnn_pbar.set_postfix({
                    'ë§¤ì¹­ë¨': len(valid_data),
                    'ëˆ„ë½': unmatched_count
                })
                    
            except Exception as e:
                print(f"âš ï¸ CNN íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {cnn_file}: {e}")
                continue
        
        print(f"âœ… {category} ìœ íš¨í•œ ë°ì´í„°: {len(valid_data)}ê°œ")
        return valid_data
    
    def prepare_attribute_data(self, category_data, attribute_name):
        """íŠ¹ì • ì†ì„±ì— ëŒ€í•œ í•™ìŠµ ë°ì´í„° ì¤€ë¹„"""
        print(f"  ğŸ¯ {attribute_name} ë°ì´í„° ì¤€ë¹„...")
        
        filtered_data = []
        for item in category_data:
            item_data = item['item_data']
            
            # ì†ì„± ë§¤í•‘ (í•œê¸€ -> ì˜ì–´)
            attribute_mapping = {
                'ì¹´í…Œê³ ë¦¬': 'category',
                'ìƒ‰ìƒ': 'color',
                'í•': 'fit',
                'ì†Œì¬': 'material',
                'ê¸°ì¥': 'length',
                'ì†Œë§¤ê¸°ì¥': 'sleeve_length',
                'ë„¥ë¼ì¸': 'neckline',
                'í”„ë¦°íŠ¸': 'print'
            }
            
            # í•´ë‹¹ ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸
            if attribute_mapping.get(attribute_name) in item_data:
                attr_value = item_data[attribute_mapping[attribute_name]]
                
                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ê°’ë§Œ ì‚¬ìš©
                if isinstance(attr_value, list):
                    attr_value = attr_value[0] if attr_value else None
                
                # ë¹ˆ ê°’ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                if attr_value and str(attr_value).strip():
                    filtered_data.append({
                        'image_path': item['image_path'],
                        'label': attr_value
                    })
        
        print(f"    ğŸ“Š {attribute_name} ë°ì´í„°: {len(filtered_data)}ê°œ")
        
        if len(filtered_data) == 0:
            print(f"    âŒ {attribute_name} í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return None
        
        return filtered_data
    
    def create_data_loaders(self, filtered_data, batch_size=32, test_size=0.2):
        """ë°ì´í„° ë¡œë” ìƒì„±"""
        if not filtered_data:
            return None, None, None, None
        
        # ë¼ë²¨ ì¸ì½”ë”©
        all_labels = [item['label'] for item in filtered_data]
        label_encoder = LabelEncoder()
        encoded_labels = label_encoder.fit_transform(all_labels)
        
        # ì´ë¯¸ì§€ ê²½ë¡œì™€ ë¼ë²¨ ë¶„ë¦¬
        image_paths = [item['image_path'] for item in filtered_data]
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            image_paths, encoded_labels, test_size=test_size, random_state=42, stratify=encoded_labels
        )
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        train_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(10),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        test_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = CategoryAttributeDataset(X_train, y_train, transform=train_transform)
        test_dataset = CategoryAttributeDataset(X_test, y_test, transform=test_transform)
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
        
        print(f"    ğŸ“ˆ í•™ìŠµ ë°ì´í„°: {len(X_train)}ê°œ")
        print(f"    ğŸ“ˆ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ")
        print(f"    ğŸ“ˆ í´ë˜ìŠ¤ ìˆ˜: {len(label_encoder.classes_)}ê°œ")
        
        return train_loader, test_loader, label_encoder, len(label_encoder.classes_)
    
    def train_attribute_model(self, train_loader, test_loader, num_classes, category, attribute, epochs=20, learning_rate=0.001):
        """ì†ì„±ë³„ ëª¨ë¸ í•™ìŠµ"""
        print(f"    ğŸš€ {category}_{attribute} ëª¨ë¸ í•™ìŠµ ì‹œì‘ (í´ë˜ìŠ¤ ìˆ˜: {num_classes})")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        model = CategoryAttributeCNN(num_classes, pretrained=True).to(DEVICE)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)
        
        # í•™ìŠµ ê¸°ë¡
        train_losses = []
        train_accuracies = []
        val_losses = []
        val_accuracies = []
        best_val_acc = 0.0
        
        # Early stopping ì„¤ì •
        patience = 3
        patience_counter = 0
        
        for epoch in range(epochs):
            print(f"\n    ğŸ“š Epoch {epoch+1}/{epochs} ì‹œì‘")
            
            # í•™ìŠµ ë‹¨ê³„
            model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            train_pbar = tqdm(train_loader, desc=f"  ğŸƒ í•™ìŠµ ì¤‘", leave=True, ncols=100)
            for batch_idx, (images, labels) in enumerate(train_pbar):
                images, labels = images.to(DEVICE), labels.to(DEVICE)
                
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()
                
                # ì§„í–‰ë°” ì—…ë°ì´íŠ¸
                current_acc = 100 * train_correct / train_total
                train_pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{current_acc:.2f}%'
                })
            
            # ê²€ì¦ ë‹¨ê³„
            model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            val_pbar = tqdm(test_loader, desc=f"  ğŸ” ê²€ì¦ ì¤‘", leave=False, ncols=100)
            with torch.no_grad():
                for images, labels in val_pbar:
                    images, labels = images.to(DEVICE), labels.to(DEVICE)
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                    
                    val_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    val_total += labels.size(0)
                    val_correct += (predicted == labels).sum().item()
                    
                    # ê²€ì¦ ì§„í–‰ë°” ì—…ë°ì´íŠ¸
                    current_val_acc = 100 * val_correct / val_total if val_total > 0 else 0
                    val_pbar.set_postfix({
                        'Val_Loss': f'{loss.item():.4f}',
                        'Val_Acc': f'{current_val_acc:.2f}%'
                    })
            
            # í†µê³„ ê³„ì‚°
            train_loss /= len(train_loader)
            train_acc = 100 * train_correct / train_total
            val_loss /= len(test_loader)
            val_acc = 100 * val_correct / val_total
            
            train_losses.append(train_loss)
            train_accuracies.append(train_acc)
            val_losses.append(val_loss)
            val_accuracies.append(val_acc)
            
            # ì—í¬í¬ ê²°ê³¼ ì¶œë ¥
            print(f"    ğŸ“Š Epoch {epoch+1} ê²°ê³¼:")
            print(f"      í•™ìŠµ Loss: {train_loss:.4f}, ì •í™•ë„: {train_acc:.2f}%")
            print(f"      ê²€ì¦ Loss: {val_loss:.4f}, ì •í™•ë„: {val_acc:.2f}%")
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                
                # ëª¨ë¸ ì €ì¥
                model_filename = f'best_model_{category}_{attribute}.pth'
                torch.save(model.state_dict(), self.output_dir / model_filename)
                print(f"      ğŸ’¾ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥! (ì •í™•ë„: {val_acc:.2f}%)")
                
            else:
                patience_counter += 1
                print(f"      â³ ì„±ëŠ¥ ê°œì„  ì—†ìŒ ({patience_counter}/{patience})")
            
            scheduler.step()
            
            # ì–¼ë¦¬ìŠ¤íƒ€í•‘ ì²´í¬
            if patience_counter >= patience:
                print(f"    âš ï¸ Early stopping triggered! (3ë²ˆ ì—°ì† ê°œì„  ì—†ìŒ)")
                break
        
        print(f"    âœ… {category}_{attribute} í•™ìŠµ ì™„ë£Œ! ìµœê³  ì •í™•ë„: {best_val_acc:.2f}%")
        
        return model, best_val_acc, train_losses, train_accuracies, val_losses, val_accuracies
    
    def train_category_models(self, category, sample_ratio=1.0):
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  ì†ì„± ëª¨ë¸ í•™ìŠµ"""
        print(f"\n{'='*60}")
        print(f"ğŸ¯ {category} ì¹´í…Œê³ ë¦¬ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print(f"{'='*60}")
        
        # ì¹´í…Œê³ ë¦¬ ë°ì´í„° ë¡œë“œ
        category_data = self.load_category_data(category)
        if not category_data:
            return None
        
        # ìƒ˜í”Œë§ ì ìš©
        if sample_ratio < 1.0:
            import random
            random.seed(42)
            sample_size = int(len(category_data) * sample_ratio)
            category_data = random.sample(category_data, sample_size)
            print(f"ğŸ“Š ìƒ˜í”Œë§ ì ìš©: {sample_size}ê°œ ë°ì´í„° ì„ íƒ ({sample_ratio*100:.1f}%)")
        
        # ì†ì„±ë³„ ëª¨ë¸ í•™ìŠµ
        attributes = ['ì¹´í…Œê³ ë¦¬', 'ìƒ‰ìƒ', 'í•', 'ì†Œì¬', 'ê¸°ì¥', 'ì†Œë§¤ê¸°ì¥', 'ë„¥ë¼ì¸', 'í”„ë¦°íŠ¸']
        category_results = {}
        
        for attribute in attributes:
            try:
                # ì†ì„± ë°ì´í„° ì¤€ë¹„
                filtered_data = self.prepare_attribute_data(category_data, attribute)
                if not filtered_data:
                    continue
                
                # ë°ì´í„° ë¡œë” ìƒì„±
                data_loaders = self.create_data_loaders(filtered_data, batch_size=32)
                if data_loaders[0] is None:
                    continue
                
                train_loader, test_loader, label_encoder, num_classes = data_loaders
                
                # ë¼ë²¨ ì¸ì½”ë” ì €ì¥
                self.label_encoders[f'{category}_{attribute}'] = label_encoder
                
                # ëª¨ë¸ í•™ìŠµ
                model, best_acc, train_losses, train_accs, val_losses, val_accs = self.train_attribute_model(
                    train_loader, test_loader, num_classes, category, attribute, epochs=10
                )
                
                category_results[attribute] = best_acc
                
                # ëª¨ë¸ ì •ë³´ ì €ì¥
                self.save_model_info(category, attribute, num_classes, best_acc, label_encoder)
                
            except Exception as e:
                print(f"âŒ {category}_{attribute} í•™ìŠµ ì‹¤íŒ¨: {e}")
                continue
        
        return category_results
    
    def save_model_info(self, category, attribute, num_classes, best_acc, label_encoder):
        """ëª¨ë¸ ì •ë³´ ì €ì¥"""
        model_filename = f'best_model_{category}_{attribute}.pth'
        info_filename = f'model_info_{category}_{attribute}.json'
        
        model_info = {
            'category': category,
            'attribute': attribute,
            'num_classes': num_classes,
            'class_names': label_encoder.classes_.tolist(),
            'best_accuracy': best_acc,
            'model_path': str(self.output_dir / model_filename)
        }
        
        with open(self.output_dir / info_filename, 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)
    
    def run_training(self, sample_ratio=0.3):
        """ì „ì²´ í•™ìŠµ ì‹¤í–‰"""
        print("ğŸš€ ì¹´í…Œê³ ë¦¬ë³„ ì†ì„± ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
        print("=" * 60)
        print(f"ğŸš€ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {DEVICE}")
        print(f"ğŸ“Š ë°ì´í„° ìƒ˜í”Œë§: {sample_ratio*100:.1f}%")
        
        categories = ['ìƒì˜', 'í•˜ì˜', 'ì•„ìš°í„°', 'ì›í”¼ìŠ¤']
        
        for category in categories:
            try:
                category_results = self.train_category_models(category, sample_ratio)
                if category_results:
                    self.training_results[category] = category_results
                    print(f"âœ… {category} í•™ìŠµ ì™„ë£Œ!")
                else:
                    print(f"âŒ {category} í•™ìŠµ ì‹¤íŒ¨!")
                    
            except Exception as e:
                print(f"âŒ {category} í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ê²°ê³¼ ìš”ì•½
        self.print_training_summary()
        
        # ê²°ê³¼ íŒŒì¼ ì €ì¥
        self.save_training_summary()
    
    def print_training_summary(self):
        """í•™ìŠµ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print("ğŸ“Š ì „ì²´ í•™ìŠµ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*60}")
        
        for category, attributes in self.training_results.items():
            print(f"\n{category}:")
            print("-" * 30)
            for attribute, acc in attributes.items():
                print(f"  {attribute:12s}: {acc:6.2f}%")
    
    def save_training_summary(self):
        """í•™ìŠµ ê²°ê³¼ ìš”ì•½ íŒŒì¼ ì €ì¥"""
        summary_file = self.output_dir / 'training_summary.json'
        
        summary_data = {
            'training_results': self.training_results,
            'output_directory': str(self.output_dir),
            'device': str(DEVICE),
            'categories': ['ìƒì˜', 'í•˜ì˜', 'ì•„ìš°í„°', 'ì›í”¼ìŠ¤'],
            'attributes': ['ì¹´í…Œê³ ë¦¬', 'ìƒ‰ìƒ', 'í•', 'ì†Œì¬', 'ê¸°ì¥', 'ì†Œë§¤ê¸°ì¥', 'ë„¥ë¼ì¸', 'í”„ë¦°íŠ¸']
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ í•™ìŠµ ê²°ê³¼ ìš”ì•½ ì €ì¥: {summary_file}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ì¹´í…Œê³ ë¦¬ë³„ ì†ì„± ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ")
    print("=" * 60)
    
    # í•™ìŠµê¸° ì´ˆê¸°í™”
    trainer = CategoryAttributeTrainer()
    
    # í•™ìŠµ ì‹¤í–‰ (10% ìƒ˜í”Œë§ìœ¼ë¡œ ë¹ ë¥¸ ì‹¤í—˜)
    trainer.run_training(sample_ratio=0.1)

if __name__ == "__main__":
    main()
